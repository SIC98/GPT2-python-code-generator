{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('finetune/checkpoint-116000')\n",
    "model = GPT2LMHeadModel.from_pretrained('finetune/checkpoint-116000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# coding=utf-8\n",
      "# Copyright 2020 The HuggingFace Inc. team. All rights reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\"\"\"Tests for the XLM-Net binary classification head.\"\"\"\n",
      "\n",
      "import unittest\n",
      "from unittest.mock import patch\n",
      "\n",
      "import pytest\n",
      "\n",
      "from transformers import XLMConfig, XLMModel, XLMTrainingArguments, XLMTokenizer, XLMModelForSeq2SeqLM\n",
      "from transformers.testing_utils import require_torch, slow, torch_device, torch_cuda\n",
      "\n",
      "\n",
      "@pytest.fixture(autouse=True)\n",
      "def input_reader():\n",
      "    \"\"\"InputReader.\n",
      "\n",
      "    Args:\n",
      "        input_reader (:obj:`TFRecordReader`):\n",
      "            The input reader to use for XLM.\n",
      "        input_reader_config (:obj:`TFSequenceClassifierConfig`):\n",
      "            The input reader config to use for XLM.\n",
      "    \"\"\"\n",
      "    config = XLMConfig()\n",
      "    config.output_hidden_states = True\n",
      "    config.output_attentions = True\n",
      "    config.use_cache = True\n",
      "\n",
      "    input_ids = input_reader_config.input_ids\n",
      "    token_type_ids = token_reader_config.token_type_ids\n",
      "    sequence_labels = token_reader_config.sequence_labels\n",
      "    token_labels = token_reader_config.token_labels\n",
      "\n",
      "    input_ids = input_reader_config.input_ids\n",
      "    input_mask = input_reader_config.input_mask\n",
      "    sequence_labels = input_reader_config.sequence_labels\n",
      "    token_labels = token_reader_config.token_labels\n",
      "\n",
      "    if not isinstance(input_ids, tf.Tensor):\n",
      "        raise TypeError(\"Input IDs should be of type tf.Tensor.\")\n",
      "\n",
      "    input_shape = input_reader_config.input_shape\n",
      "    input_ids = input_ids.view(-1, input_shape[-1] + 1, input_shape[-1] + 1)\n",
      "    input_mask = input_reader_config.input_mask.view(-1, input_shape[-1] + 1, input_shape[-1] + 1)\n",
      "    sequence_labels = input_reader_config.sequence_labels.view(-1, sequence_labels.view(-1))\n",
      "    token_labels = input_reader_config.token_labels.view(-1, token_labels.view(-1))\n",
      "\n",
      "    output_from_no_past = input_reader_config.output_from_no_past\n",
      "    output_from_past = input_reader_config.output_from_past\n",
      "\n",
      "    if not isinstance(input_ids, tf.Tensor):\n",
      "        raise TypeError(\"Input IDs should be of type tf.Tensor.\")\n",
      "\n",
      "    input_shape = input_reader_config.input_shape\n",
      "    input_ids = input_ids.view(-1, input_shape[-1] + 1, input_shape[-1] + 1)\n",
      "    input_mask = input_reader_config.input_mask.view(-1, input_shape[-1] + 1, input_shape[-1] + 1)\n",
      "    sequence_labels = input_reader_config.sequence_labels.view(-1, sequence_labels.view(-1))\n",
      "    token_labels = input_reader_config.token_labels.view(-1\n"
     ]
    }
   ],
   "source": [
    "sequence = \"\"\"# coding=utf-8\n",
    "# Copyright 2020 The HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\"\"\"\n",
    "inputs = tokenizer.encode(sequence, return_tensors='pt')\n",
    "outputs = model.generate(inputs, max_length=1024, do_sample=True, temperature=0.5, top_p=1.0)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def is_palindrome(s):\n",
      "    \"\"\"Check whether a string is a palindrome\"\"\"\n",
      "    return (s == '\\n')\n",
      "\n",
      "\n",
      "def is_palindrome_unicode(s):\n",
      "    \"\"\"Check whether a string is a palindrome\n"
     ]
    }
   ],
   "source": [
    "sequence = 'def is_palindrome(s):\\n    \"\"\"Check whether a string is a palindrome\"\"\"'\n",
    "    \n",
    "inputs = tokenizer.encode(sequence, return_tensors='pt')\n",
    "outputs = model.generate(inputs, max_length=64, do_sample=True, temperature=0.1, top_p=1.0)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def long_palindrome_indices(l):\n",
      "    \"\"\"Return list indices for elemets that are palindrimes and at least 7 characters\"\"\"\n",
      "    return [l[i] for i in range(l.shape[0])]\n",
      "\n",
      "\n",
      "def long_pal\n"
     ]
    }
   ],
   "source": [
    "sequence = 'def long_palindrome_indices(l):\\n    \"\"\"Return list indices for elemets that are palindrimes and at least 7 characters\"\"\"'\n",
    "    \n",
    "inputs = tokenizer.encode(sequence, return_tensors='pt')\n",
    "outputs = model.generate(inputs, max_length=64, do_sample=True, temperature=0.1, top_p=1.0)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@dataclass\n",
      "class Item:\n",
      "    name: str\n",
      "    price: float\n",
      "\n",
      "@dataclass\n",
      "class Order\n",
      "    id: int\n",
      "    items: List[Item]\n",
      "    \n",
      "    def compute_total_price(self, palindrome_discount=0.2):\n",
      "        return self.calculate_total_price(palindrome_discount)\n",
      "\n",
      "\n",
      "class OrderBy:\n",
      "    name: str\n",
      "    price: float\n",
      "\n",
      "@datac\n"
     ]
    }
   ],
   "source": [
    "sequence = \"\"\"@dataclass\n",
    "class Item:\n",
    "    name: str\n",
    "    price: float\n",
    "\n",
    "@dataclass\n",
    "class Order\n",
    "    id: int\n",
    "    items: List[Item]\n",
    "    \n",
    "    def compute_total_price(self, palindrome_discount=0.2):\"\"\"\n",
    "    \n",
    "inputs = tokenizer.encode(sequence, return_tensors='pt')\n",
    "outputs = model.generate(inputs, max_length=128, do_sample=True, temperature=0.1, top_p=1.0)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@dataclass\n",
      "class Item:\n",
      "    name: str\n",
      "    price: float\n",
      "\n",
      "@dataclass\n",
      "class Order\n",
      "    id: int\n",
      "    items: List[Item]\n",
      "    \n",
      "    def compute_total_price(self, palindrome_discount=0.2):\n",
      "        \"\"\"\n",
      "        Compute the total price and return it.\n",
      "        Apply a discount to items whose names are palindromes.\n",
      "        \"\"\"\n",
      "        return self.calculate_total_price(palindrome_discount)\n",
      "\n",
      "    def calculate_total_price(self, palindrome_discount):\n",
      "        \"\"\"\n",
      "        Calculate the total price and return it.\n",
      "        Apply a discount to items whose names are palindromes.\n",
      "        \"\"\"\n",
      "        return self.calculate_total_price(palind\n"
     ]
    }
   ],
   "source": [
    "sequence = '@dataclass\\nclass Item:\\n    name: str\\n    price: float\\n\\n@dataclass\\nclass Order\\n    id: int\\n    items: List[Item]\\n    \\n    def compute_total_price(self, palindrome_discount=0.2):\\n        \"\"\"\\n        Compute the total price and return it.\\n        Apply a discount to items whose names are palindromes.\\n        \"\"\"'\n",
    "    \n",
    "inputs = tokenizer.encode(sequence, return_tensors='pt')\n",
    "outputs = model.generate(inputs, max_length=256, do_sample=True, temperature=0.2, top_p=1.0)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def add_all(a, b, c, d):\n",
      "        \"\"\"\n",
      "        Add all elements to the list of elements in the list\n",
      "        \"\"\"\n",
      "        a = a.copy()\n",
      "        b = b.copy()\n",
      "        c = c.copy()\n",
      "        d = d.copy()\n",
      "        return a, b, c, d\n",
      "\n",
      "    def add_all(a, b, c, d):\n",
      "        \"\"\"\n",
      "        Add all elements to the list of elements in the list\n",
      "        \"\"\"\n",
      "        a = a.copy()\n",
      "        b = b.copy()\n",
      "        c = c.copy()\n",
      "        d = d.copy()\n",
      "        return a, b, c, d\n",
      "\n",
      "    def add_all\n"
     ]
    }
   ],
   "source": [
    "sequence = 'def add_all(a, b, c, d):'\n",
    "    \n",
    "inputs = tokenizer.encode(sequence, return_tensors='pt')\n",
    "outputs = model.generate(inputs, max_length=256, do_sample=True, temperature=0.2, top_p=1.0)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python transformers/examples/text-generation/run_generation.py \\\n",
    "    --model_type=gpt2 \\\n",
    "    --model_name_or_path='finetune/checkpoint-116000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
