{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e9d2a0f66940e2ad981d6ed3b7f624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=798156.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08010fe5df1d4e78a7bb8853a8f1e42c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456356.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2c78173165493ebaa72ee684d3bc24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=90.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a09ca28d5b24357b2ef2fd01bebd714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=167.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ffabc8bebf841148231a39a630d70a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=803.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d2a757d05bb433a88a7c454d9569855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=510406265.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('SIC98/GPT2-python-code-generator')\n",
    "model = GPT2LMHeadModel.from_pretrained('SIC98/GPT2-python-code-generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can use fast tokenizer\n",
    "from transformers import GPT2TokenizerFast\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('SIC98/GPT2-python-code-generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [15496, 995], 'attention_mask': [1, 1]}\n",
      "{'input_ids': [18435, 995], 'attention_mask': [1, 1]}\n",
      "[15496, 995]\n",
      "[18435, 995]\n",
      "Hello world\n",
      " Hello world\n"
     ]
    }
   ],
   "source": [
    "# Test tokenizer\n",
    "print(tokenizer(\"Hello world\"))\n",
    "print(tokenizer(\" Hello world\"))\n",
    "\n",
    "print(tokenizer.encode(\"Hello world\"))\n",
    "print(tokenizer.encode(\" Hello world\"))\n",
    "\n",
    "print(tokenizer.decode([15496, 995]))\n",
    "print(tokenizer.decode([18435, 995]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# coding=utf-8\n",
      "# Copyright 2020 The HuggingFace Inc. team. All rights reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "from typing import Any, Dict, List, Optional, Tuple\n",
      "\n",
      "from...file_utils import is_sentencepiece_available\n",
      "from...utils import logging\n",
      "\n",
      "\n",
      "logger = logging.get_logger(__name__)\n",
      "\n",
      "VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\"}\n",
      "\n",
      "PRETRAINED_VOCAB_FILES_MAP = {\n",
      "    \"vocab_file\": {\n",
      "        \"gpt2-base\": \"https://huggingface.co/gpt2-base/resolve/main/vocab.txt\",\n",
      "        \"gpt2-large\": \"https://huggingface.co/gpt2-large/resolve/main/vocab.txt\",\n",
      "        \"gpt2-xlm-base\": \"https://huggingface.co/gpt2-xlm-base/resolve/main/vocab.txt\",\n",
      "        \"gpt2-xlm-large\": \"https://huggingface.co/gpt2-xlm-large/resolve/main/vocab.txt\",\n",
      "        \"gpt2-xlm-head\": \"https://huggingface.co/gpt2-xlm-head/resolve/main/vocab.txt\",\n",
      "        \"gpt2-xlm-head\": \"https://huggingface.co/gpt2-xlm-head/resolve/main/vocab.txt\",\n",
      "        \"gpt2-xlm-decoder\": \"https://huggingface.co/gpt2-xlm-decoder/resolve/main/vocab.txt\",\n",
      "        \"gpt2-xlm-head\": \"https://huggingface.co/gpt2-xlm-head/resolve/main/vocab.txt\",\n",
      "        \"gpt2-xlm-decoder\": \"https://huggingface.co/gpt2-xlm-decoder/resolve/main/vocab.txt\",\n",
      "        \"gpt2-xlm-head\": \"https://huggingface.co/gpt2-xlm-head/resolve/main/vocab.txt\",\n",
      "        \"gpt2-xlm-decoder\": \"https://huggingface.co/gpt2-xlm-decoder/resolve/main/vocab.txt\",\n",
      "        \"gpt2-xlm-head\": \"https://huggingface.co/gpt2-xlm-head/resolve/main/vocab.txt\",\n",
      "        \"gpt2-xlm-decoder\": \"https://huggingface.co/gpt2-xlm-decoder/resolve/main/vocab.txt\",\n",
      "        \"gpt2-xlm-head\": \"https://huggingface.co/gpt2-xlm-head/resolve/main/vocab.txt\",\n",
      "        \"gpt2-xlm-decoder\": \"https://huggingface.co/gpt2-xlm-decoder/resolve/main/vocab.txt\",\n",
      "        \"gpt2-xlm-head\": \"https://hugging\n"
     ]
    }
   ],
   "source": [
    "sequence = \"\"\"# coding=utf-8\n",
    "# Copyright 2020 The HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\"\"\"\n",
    "inputs = tokenizer.encode(sequence, return_tensors='pt')\n",
    "outputs = model.generate(inputs, max_length=1024, do_sample=True, temperature=0.5, top_p=1.0)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def is_palindrome(s):\n",
      "    \"\"\"Check whether a string is a palindrome\"\"\"\n",
      "    return (s == '\\n') and (s == '\\n')\n",
      "\n",
      "\n",
      "def is_palindrome_unicode(s):\n",
      "    \"\"\"Check\n"
     ]
    }
   ],
   "source": [
    "sequence = 'def is_palindrome(s):\\n    \"\"\"Check whether a string is a palindrome\"\"\"'\n",
    "    \n",
    "inputs = tokenizer.encode(sequence, return_tensors='pt')\n",
    "outputs = model.generate(inputs, max_length=64, do_sample=True, temperature=0.1, top_p=1.0)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def long_palindrome_indices(l):\n",
      "    \"\"\"Return list indices for elemets that are palindrimes and at least 7 characters\"\"\"\n",
      "    return [l[i] for i in range(l.shape[0])]\n",
      "\n",
      "\n",
      "def long_pal\n"
     ]
    }
   ],
   "source": [
    "sequence = 'def long_palindrome_indices(l):\\n    \"\"\"Return list indices for elemets that are palindrimes and at least 7 characters\"\"\"'\n",
    "    \n",
    "inputs = tokenizer.encode(sequence, return_tensors='pt')\n",
    "outputs = model.generate(inputs, max_length=64, do_sample=True, temperature=0.1, top_p=1.0)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@dataclass\n",
      "class Item:\n",
      "    name: str\n",
      "    price: float\n",
      "\n",
      "@dataclass\n",
      "class Order\n",
      "    id: int\n",
      "    items: List[Item]\n",
      "    \n",
      "    def compute_total_price(self, palindrome_discount=0.2):\n",
      "        return self.calculate_total_price(palindrome_discount)\n",
      "\n",
      "    def calculate_total_price(self, palindrome_discount=0.2):\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sequence = \"\"\"@dataclass\n",
    "class Item:\n",
    "    name: str\n",
    "    price: float\n",
    "\n",
    "@dataclass\n",
    "class Order\n",
    "    id: int\n",
    "    items: List[Item]\n",
    "    \n",
    "    def compute_total_price(self, palindrome_discount=0.2):\"\"\"\n",
    "    \n",
    "inputs = tokenizer.encode(sequence, return_tensors='pt')\n",
    "outputs = model.generate(inputs, max_length=128, do_sample=True, temperature=0.1, top_p=1.0)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@dataclass\n",
      "class Item:\n",
      "    name: str\n",
      "    price: float\n",
      "\n",
      "@dataclass\n",
      "class Order\n",
      "    id: int\n",
      "    items: List[Item]\n",
      "    \n",
      "    def compute_total_price(self, palindrome_discount=0.2):\n",
      "        \"\"\"\n",
      "        Compute the total price and return it.\n",
      "        Apply a discount to items whose names are palindromes.\n",
      "        \"\"\"\n",
      "        return self.calculate_total_price(palindrome_discount)\n",
      "\n",
      "    def calculate_total_price(self, palindrome_discount):\n",
      "        \"\"\"\n",
      "        Calculate the total price and return it.\n",
      "        Apply a discount to items whose names are palindromes.\n",
      "        \"\"\"\n",
      "        return self.calculate_total_price(palind\n"
     ]
    }
   ],
   "source": [
    "sequence = '@dataclass\\nclass Item:\\n    name: str\\n    price: float\\n\\n@dataclass\\nclass Order\\n    id: int\\n    items: List[Item]\\n    \\n    def compute_total_price(self, palindrome_discount=0.2):\\n        \"\"\"\\n        Compute the total price and return it.\\n        Apply a discount to items whose names are palindromes.\\n        \"\"\"'\n",
    "    \n",
    "inputs = tokenizer.encode(sequence, return_tensors='pt')\n",
    "outputs = model.generate(inputs, max_length=256, do_sample=True, temperature=0.2, top_p=1.0)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def add_all(a, b, c, d):\n",
      "    \"\"\"\n",
      "    Adds all the elements of a list to the list of elements in a list.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    a : list\n",
      "        List of elements to add to the list.\n",
      "    b : list\n",
      "        List of elements to add to the list.\n",
      "    c : list\n",
      "        List of elements to add to the list.\n",
      "    d : list\n",
      "        List of elements to add to the list.\n",
      "    \"\"\"\n",
      "    a = [a]\n",
      "    b = [b]\n",
      "    c = [c]\n",
      "    d = [d]\n",
      "    if len(a) == 2:\n",
      "        a = [a]\n",
      "        b = [b]\n",
      "        c = [c]\n",
      "        d = [d]\n",
      "    if len\n"
     ]
    }
   ],
   "source": [
    "sequence = 'def add_all(a, b, c, d):'\n",
    "    \n",
    "inputs = tokenizer.encode(sequence, return_tensors='pt')\n",
    "outputs = model.generate(inputs, max_length=256, do_sample=True, temperature=0.2, top_p=1.0)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python transformers/examples/text-generation/run_generation.py \\\n",
    "    --model_type=gpt2 \\\n",
    "    --model_name_or_path='SIC98/GPT2-python-code-generator'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
