{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_dict = {'bos_token': '<BOS>', 'eos_token': '<EOS>', 'pad_token': '<PAD>'}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_added_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/20/2021 00:18:39 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "01/20/2021 00:18:39 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=finetune, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.EPOCH, prediction_loss_only=False, per_device_train_batch_size=5, per_device_eval_batch_size=5, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Jan20_00-18-39_sic-X570-AORUS-MASTER, logging_first_step=True, logging_steps=500, save_steps=1000, save_total_limit=10, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=finetune, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, _n_gpu=1)\n",
      "Using custom data configuration default\n",
      "Reusing dataset text (/home/sic/.cache/huggingface/datasets/text/default-d2d9150f2259a418/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab)\n",
      "[INFO|configuration_utils.py:445] 2021-01-20 00:18:42,474 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /home/sic/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "[INFO|configuration_utils.py:481] 2021-01-20 00:18:42,475 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.2.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:445] 2021-01-20 00:18:43,355 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /home/sic/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "[INFO|configuration_utils.py:481] 2021-01-20 00:18:43,356 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.2.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1766] 2021-01-20 00:18:45,999 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /home/sic/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "[INFO|tokenization_utils_base.py:1766] 2021-01-20 00:18:45,999 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /home/sic/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1766] 2021-01-20 00:18:45,999 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /home/sic/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "[INFO|modeling_utils.py:1027] 2021-01-20 00:18:46,903 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /home/sic/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
      "[INFO|modeling_utils.py:1143] 2021-01-20 00:18:49,597 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:1151] 2021-01-20 00:18:49,597 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Loading cached processed dataset at /home/sic/.cache/huggingface/datasets/text/default-d2d9150f2259a418/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-b908befe2d45417a.arrow\n",
      "  7%|██▊                                       | 54/814 [00:00<00:07, 97.08ba/s][WARNING|tokenization_utils_base.py:3194] 2021-01-20 00:18:50,232 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1775 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|████████████████████████████████████████| 814/814 [00:07<00:00, 115.23ba/s]\n",
      "Loading cached processed dataset at /home/sic/.cache/huggingface/datasets/text/default-d2d9150f2259a418/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-29ca61c926af7986.arrow\n",
      "100%|█████████████████████████████████████████| 814/814 [00:20<00:00, 40.42ba/s]\n",
      "[INFO|trainer.py:441] 2021-01-20 00:19:18,476 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
      "[INFO|trainer.py:441] 2021-01-20 00:19:18,476 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
      "[INFO|trainer.py:358] 2021-01-20 00:19:18,476 >> Using amp fp16 backend\n",
      "[INFO|trainer.py:791] 2021-01-20 00:19:18,478 >> ***** Running training *****\n",
      "[INFO|trainer.py:792] 2021-01-20 00:19:18,478 >>   Num examples = 109505\n",
      "[INFO|trainer.py:793] 2021-01-20 00:19:18,478 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:794] 2021-01-20 00:19:18,478 >>   Instantaneous batch size per device = 5\n",
      "[INFO|trainer.py:795] 2021-01-20 00:19:18,478 >>   Total train batch size (w. parallel, distributed & accumulation) = 5\n",
      "[INFO|trainer.py:796] 2021-01-20 00:19:18,478 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:797] 2021-01-20 00:19:18,478 >>   Total optimization steps = 109505\n",
      "  0%|                                                | 0/109505 [00:00<?, ?it/s]/home/sic/Documents/projects/GPT2-python-code-generator/venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "{'loss': 1.6911, 'learning_rate': 4.9999543399844754e-05, 'epoch': 0.0}         \n",
      "{'loss': 1.5093, 'learning_rate': 4.977169992237797e-05, 'epoch': 0.02}         \n",
      "{'loss': 1.4077, 'learning_rate': 4.954339984475595e-05, 'epoch': 0.05}         \n",
      "  1%|▎                                  | 1000/109505 [05:09<9:09:52,  3.29it/s][INFO|trainer.py:1344] 2021-01-20 00:24:28,332 >> Saving model checkpoint to finetune/checkpoint-1000\n",
      "[INFO|configuration_utils.py:300] 2021-01-20 00:24:28,332 >> Configuration saved in finetune/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:817] 2021-01-20 00:24:28,835 >> Model weights saved in finetune/checkpoint-1000/pytorch_model.bin\n",
      "{'loss': 1.3413, 'learning_rate': 4.9315099767133924e-05, 'epoch': 0.07}        \n",
      "{'loss': 1.3044, 'learning_rate': 4.90867996895119e-05, 'epoch': 0.09}          \n",
      "  2%|▋                                  | 2000/109505 [10:20<9:05:22,  3.29it/s][INFO|trainer.py:1344] 2021-01-20 00:29:38,793 >> Saving model checkpoint to finetune/checkpoint-2000\n",
      "[INFO|configuration_utils.py:300] 2021-01-20 00:29:38,793 >> Configuration saved in finetune/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:817] 2021-01-20 00:29:39,294 >> Model weights saved in finetune/checkpoint-2000/pytorch_model.bin\n",
      "{'loss': 1.2708, 'learning_rate': 4.885849961188987e-05, 'epoch': 0.11}         \n",
      "  2%|▊                                  | 2590/109505 [13:22<9:02:17,  3.29it/s]"
     ]
    }
   ],
   "source": [
    "!python transformers/examples/language-modeling/run_clm.py \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --fp16 \\\n",
    "    --logging_first_step \\\n",
    "    --evaluation_strategy epoch \\\n",
    "    --logging_steps 500 \\\n",
    "    --model_name_or_path gpt2 \\\n",
    "    --model_type gpt2 \\\n",
    "    --num_train_epochs 5 \\\n",
    "    --output_dir finetune \\\n",
    "    --per_device_eval_batch_size 5 \\\n",
    "    --per_device_train_batch_size 5 \\\n",
    "    --save_steps 10000 \\\n",
    "    --save_total_limit 10 \\\n",
    "    --train_file data/train.txt \\\n",
    "    --validation_file data/test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
